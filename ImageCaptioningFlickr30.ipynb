{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "ImageCaptioningFlickr30.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duttagoutam/LJMUMasters/blob/master/ImageCaptioningFlickr30.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTEpDQdf7kQN",
        "colab_type": "text"
      },
      "source": [
        "# Image Captioning Flickr8k Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRNGEmrTHQZN",
        "colab_type": "code",
        "outputId": "1be58d5a-efa6-44d8-e8f2-df48f7e787a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBHosj4B7kQS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import string\n",
        "import sys, time, os, warnings \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pydotplus\n",
        "from array import array\n",
        "from os import listdir\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Activation\n",
        "from keras.layers.merge import add\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from numpy import argmax\n",
        "from pickle import dump\n",
        "from pickle import load\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "#keras.utils.vis_utils.pydot = pydot\n",
        "#from keras.backend.tensorflow_backend import set_session\n",
        "#from collections import Counter "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sTjBmR3Sr2C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "folderName='/content/drive/My Drive/App/storage/flickr30k_images'\n",
        "imageDir=folderName+'/Images'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB5R8JOv7kQf",
        "colab_type": "text"
      },
      "source": [
        "## Load Photographs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQxsFP7S7kQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Not To Run Always\n",
        "npix = 224\n",
        "target_size = (npix,npix,3)\n",
        "\n",
        "def load_photos(directory):\n",
        "    images = dict()\n",
        "    for name in listdir(directory):\n",
        "        # load an image from file\n",
        "        #print(\"name\",name)\n",
        "        filename = directory + '/' + name\n",
        "        image = load_img(filename, target_size)\n",
        "        # convert the image pixels to a numpy array\n",
        "        image = img_to_array(image)\n",
        "        # reshape data for the model\n",
        "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "        # prepare the image for the VGG model\n",
        "        image = preprocess_input(image)\n",
        "        # get image id\n",
        "        image_id = name.split('.')[0]\n",
        "        images[image_id] = image\n",
        "    return images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUtyor3l7kQv",
        "colab_type": "code",
        "outputId": "72fba2ea-1e5a-4e51-eb47-03b59258a61a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "# Not To Run Always\n",
        "# load images\n",
        "#directory = 'Flicker8k_Dataset'\n",
        "directory = imageDir\n",
        "images = load_photos(directory)\n",
        "print('Loaded Images: %d' % len(images))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-ba2474b92e5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimageDir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_photos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loaded Images: %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-9f3c0c0d57ad>\u001b[0m in \u001b[0;36mload_photos\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m#print(\"name\",name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;31m# convert the image pixels to a numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    108\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[1;32m    109\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'L'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2537\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2539\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2541\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qotlxBO7kQ1",
        "colab_type": "text"
      },
      "source": [
        "## Extract Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7KJzMmU7kQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Not To Run Always\n",
        "# extract features from each photo in the directory\n",
        "def extract_features(directory):\n",
        "    in_layer = Input(shape=(224, 224, 3))\n",
        "    model = VGG16(include_top=False, input_tensor=in_layer)\n",
        "    #model.layers.pop()\n",
        "    #model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
        "    print(model.summary())\n",
        "    features = dict()\n",
        "    for name in listdir(directory):\n",
        "        filename = directory + '/' + name\n",
        "        image = load_img(filename, target_size=(224, 224))\n",
        "        image = img_to_array(image)\n",
        "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "        image = preprocess_input(image)\n",
        "        feature = model.predict(image, verbose=0)\n",
        "        image_id = name.split('.')[0]\n",
        "        features[image_id] = feature\n",
        "        #print('>%s' % name)\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1lg18ha7kQ5",
        "colab_type": "code",
        "outputId": "ba0f8795-79d2-4b04-c36a-c54d5599ff13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        }
      },
      "source": [
        "# Not To Run Always\n",
        "# extract features from all images\n",
        "#directory = 'Flickr8k_Dataset'\n",
        "directory = imageDir\n",
        "features = extract_features(directory)\n",
        "print('Extracted Features: %d' % len(features))\n",
        "# save to file\n",
        "dump(features, open(folderName+'/features.pkl', 'wb'))\n",
        "print('Image Features: %d' % len(features))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCXu6ptA7kQ8",
        "colab_type": "text"
      },
      "source": [
        "## Prepare Text Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fek1Lhbm7kQ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SOsE6SaRvWS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Not To Run Always\n",
        "# extract descriptions for images\n",
        "def load_descriptions(doc):\n",
        "    mapping = dict()\n",
        "    # process lines\n",
        "    for line in doc.split('\\n'):\n",
        "        # split line by white space\n",
        "        tokens = line.split()\n",
        "        if len(line) < 2:\n",
        "            continue\n",
        "        # take the first token as the image id, the rest as the description\n",
        "        image_id, image_desc = tokens[0], tokens[1:]\n",
        "        # remove filename from image id\n",
        "        image_id = image_id.split('.')[0]\n",
        "        # convert description tokens back to string\n",
        "        image_desc = ' '.join(image_desc)\n",
        "        # create the list if needed\n",
        "        if image_id not in mapping:\n",
        "            mapping[image_id] = list()\n",
        "        # store description\n",
        "        mapping[image_id].append(image_desc)\n",
        "    return mapping"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msQo-8qLR4Gb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Not To Run Always\n",
        "def clean_descriptions(descriptions):\n",
        "    # prepare translation table for removing punctuation\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    for key, desc_list in descriptions.items():\n",
        "        for i in range(len(desc_list)):\n",
        "            desc = desc_list[i]\n",
        "            # tokenize\n",
        "            desc = desc.split()\n",
        "            # convert to lower case\n",
        "            desc = [word.lower() for word in desc]\n",
        "            # remove punctuation from each token\n",
        "            desc = [w.translate(table) for w in desc]\n",
        "            # remove hanging 's' and 'a'\n",
        "            desc = [word for word in desc if len(word)>1]\n",
        "            # remove tokens with numbers in them\n",
        "            desc = [word for word in desc if word.isalpha()]\n",
        "            # store as string\n",
        "            desc_list[i] =  ' '.join(desc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7TdjNTnR7bM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Not To Run Always\n",
        "# convert the loaded descriptions into a vocabulary of words\n",
        "def to_vocabulary(descriptions):\n",
        "    # build a list of all description strings\n",
        "    all_desc = set()\n",
        "    for key in descriptions.keys():\n",
        "        [all_desc.update(d.split()) for d in descriptions[key]]\n",
        "    return all_desc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34a5OvGMR_dJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Not To Run Always\n",
        "# save descriptions to file, one per line\n",
        "def save_descriptions(descriptions, filename):\n",
        "    lines = list()\n",
        "    for key, desc_list in descriptions.items():\n",
        "        for desc in desc_list:\n",
        "            lines.append(key + ' ' + desc)\n",
        "    data = '\\n'.join(lines)\n",
        "    file = open(filename, 'w')\n",
        "    file.write(data)\n",
        "    file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0K_hroMSDQe",
        "colab_type": "code",
        "outputId": "d1c5cd45-0d7c-4464-da7d-117793b1356c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Not To Run Always\n",
        "filename = folderName+'/Flickr_TextData/Flickr8k.token.txt'\n",
        "# load descriptions\n",
        "doc = load_doc(filename)\n",
        "# parse descriptions\n",
        "descriptions = load_descriptions(doc)\n",
        "print('Loaded: %d ' % len(descriptions))\n",
        "# clean descriptions\n",
        "clean_descriptions(descriptions)\n",
        "# summarize vocabulary\n",
        "vocabulary = to_vocabulary(descriptions)\n",
        "print('Vocabulary Size: %d' % len(vocabulary))\n",
        "# save to file\n",
        "save_descriptions(descriptions, folderName+'/descriptions.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded: 8092 \n",
            "Vocabulary Size: 8763\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjGtlLBaSbOf",
        "colab_type": "text"
      },
      "source": [
        "# Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWy27K4q7kQ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load a pre-defined list of photo identifiers\n",
        "def load_set(filename):\n",
        "    doc = load_doc(filename)\n",
        "    dataset = list()\n",
        "    # process line by line\n",
        "    for line in doc.split('\\n'):\n",
        "        # skip empty lines\n",
        "        if len(line) < 1:\n",
        "            continue\n",
        "        # get the image identifier\n",
        "        identifier = line.split('.')[0]\n",
        "        dataset.append(identifier)\n",
        "    return set(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XWjFQdx7kRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load clean descriptions into memory\n",
        "def load_clean_descriptions(filename, dataset):\n",
        "    # load document\n",
        "    doc = load_doc(filename)\n",
        "    descriptions = dict()\n",
        "    for line in doc.split('\\n'):\n",
        "        # split line by white space\n",
        "        tokens = line.split()\n",
        "        # split id from description\n",
        "        image_id, image_desc = tokens[0], tokens[1:]\n",
        "        # skip images not in the set\n",
        "        if image_id in dataset:\n",
        "            # create list\n",
        "            if image_id not in descriptions:\n",
        "                descriptions[image_id] = list()\n",
        "            # wrap description in tokens\n",
        "            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
        "            # store\n",
        "            descriptions[image_id].append(desc)\n",
        "    return descriptions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "if9FX7J17kRG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load photo features\n",
        "def load_photo_features(filename, dataset):\n",
        "    # load all features\n",
        "    all_features = load(open(filename, 'rb'))\n",
        "    # filter features\n",
        "    features = {k: all_features[k] for k in dataset}\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t29K8tC97kRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# covert a dictionary of clean descriptions to a list of descriptions\n",
        "def to_lines(descriptions):\n",
        "    all_desc = list()\n",
        "    for key in descriptions.keys():\n",
        "        [all_desc.append(d) for d in descriptions[key]]\n",
        "    return all_desc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulBuEKtq7kRN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fit a tokenizer given caption descriptions\n",
        "def create_tokenizer(descriptions):\n",
        "    lines = to_lines(descriptions)\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kO2IhtK_7kRQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate the length of the description with the most words\n",
        "def maximum_length(descriptions):\n",
        "    lines = to_lines(descriptions)\n",
        "    return max(len(d.split()) for d in lines)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_-bg7tF7kRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Not Working\n",
        "# create sequences of images, input sequences and output words for an image\n",
        "def create_sequences(tokenizer, max_length, descriptions, photos, vocab_size):\n",
        "    X1, X2, y = list(), list(), list()\n",
        "    #X1, X2, y = [], [], []\n",
        "    # walk through each image identifier\n",
        "    for key, desc_list in descriptions.items():\n",
        "        # walk through each description for the image\n",
        "        for desc in desc_list:\n",
        "            # encode the sequence\n",
        "            seq = tokenizer.texts_to_sequences([desc])[0]\n",
        "            # split one sequence into multiple X,y pairs\n",
        "            for i in range(1, len(seq)):\n",
        "                # split into input and output pair\n",
        "                in_seq, out_seq = seq[:i], seq[i]\n",
        "                # pad input sequence\n",
        "                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "                # encode output sequence\n",
        "                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "                # store\n",
        "                X1.append(photos[key][0])\n",
        "                X2.append(in_seq)\n",
        "                y.append(out_seq)\n",
        "        #X1 = np.array(X1)\n",
        "        #X2 = np.array(X2)\n",
        "        #y  = np.array(y)\n",
        "    \n",
        "    return np.array(X1), np.array(X2), np.array(y)\n",
        "    #return (X1, X2, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3y2CaP-mydM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create sequences of images, input sequences and output words for an image\n",
        "def create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):\n",
        "\tX1, X2, y = list(), list(), list()\n",
        "\t# walk through each description for the image\n",
        "\tfor desc in desc_list:\n",
        "\t\t# encode the sequence\n",
        "\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n",
        "\t\t# split one sequence into multiple X,y pairs\n",
        "\t\tfor i in range(1, len(seq)):\n",
        "\t\t\t# split into input and output pair\n",
        "\t\t\tin_seq, out_seq = seq[:i], seq[i]\n",
        "\t\t\t# pad input sequence\n",
        "\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "\t\t\t# encode output sequence\n",
        "\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "\t\t\t# store\n",
        "\t\t\tX1.append(photo)\n",
        "\t\t\tX2.append(in_seq)\n",
        "\t\t\ty.append(out_seq)\n",
        "\treturn np.array(X1), np.array(X2), np.array(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftJ-axM-7kRW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the captioning model\n",
        "def define_model(vocab_size, max_length):\n",
        "    # feature extractor model\n",
        "    inputs1 = Input(shape=(4096,))\n",
        "    fe1 = Dropout(0.5)(inputs1)\n",
        "    fe2 = Dense(256, activation='relu')(fe1)\n",
        "    # sequence model\n",
        "    inputs2 = Input(shape=(max_length,))\n",
        "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "    se2 = Dropout(0.5)(se1)\n",
        "    se3 = LSTM(256)(se2)\n",
        "    # decoder model\n",
        "    decoder1 = add([fe2, se3])\n",
        "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "    # tie it together [image, seq] [word]\n",
        "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "    # summarize model\n",
        "    print(model.summary())\n",
        "    #plot_model(model, to_file='model.png', show_shapes=True)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4Iv0XOJShHj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data generator, intended to be used in a call to model.fit_generator()\n",
        "def data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n",
        "\t# loop for ever over images\n",
        "\twhile 1:\n",
        "\t\tfor key, desc_list in descriptions.items():\n",
        "\t\t\t# retrieve the photo feature\n",
        "\t\t\tphoto = photos[key][0]\n",
        "\t\t\tin_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n",
        "\t\t\tyield [[in_img, in_seq], out_word]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Li9o9VqR7kRY",
        "colab_type": "text"
      },
      "source": [
        "## Train Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2b1YCDr7kRZ",
        "colab_type": "code",
        "outputId": "cc315db2-456c-4014-a77a-8863f6be993e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Not Working\n",
        "# load training dataset (6K)\n",
        "#filename = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
        "filename = folderName+'/Flickr_TextData/Flickr_8k.trainImages.txt'\n",
        "train = load_set(filename)\n",
        "print('Dataset: %d' % len(train))\n",
        "# descriptions\n",
        "train_descriptions = load_clean_descriptions(folderName+'/descriptions.txt', train)\n",
        "print('Descriptions: train=%d' % len(train_descriptions))\n",
        "# photo features\n",
        "train_features = load_photo_features(folderName+'/features.pkl', train)\n",
        "print('Photos: train=%d' % len(train_features))\n",
        "# prepare tokenizer\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "# determine the maximum sequence length\n",
        "max_length = maximum_length(train_descriptions)\n",
        "print('Description Length: %d' % max_length)\n",
        "# prepare sequences\n",
        "X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features, vocab_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 6000\n",
            "Descriptions: train=6000\n",
            "Photos: train=6000\n",
            "Vocabulary Size: 7579\n",
            "Description Length: 34\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-2bRqC17kRc",
        "colab_type": "text"
      },
      "source": [
        "## Test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBcsCu8g7kRc",
        "colab_type": "code",
        "outputId": "acc48293-0d6e-4c23-8f85-7c5717c7b746",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "# Not Working\n",
        "# load test set\n",
        "#filename = 'Flickr8k_text/Flickr_8k.devImages.txt'\n",
        "filename = folderName+'/Flickr_TextData/Flickr_8k.devImages.txt'\n",
        "test = load_set(filename)\n",
        "print('Dataset: %d' % len(test))\n",
        "# descriptions\n",
        "test_descriptions = load_clean_descriptions(folderName+'/descriptions.txt', test)\n",
        "print('Descriptions: test=%d' % len(test_descriptions))\n",
        "# photo features\n",
        "test_features = load_photo_features(folderName+'/features.pkl', test)\n",
        "print('Photos: test=%d' % len(test_features))\n",
        "# prepare sequences\n",
        "X1test, X2test, ytest = create_sequences(tokenizer, max_length, test_descriptions, test_features, vocab_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-911d68e8c937>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfolderName\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/Flickr_TextData/Flickr_8k.devImages.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dataset: %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# descriptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_descriptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_clean_descriptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolderName\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/descriptions.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'folderName' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXTwMVY87kRf",
        "colab_type": "text"
      },
      "source": [
        "## Fit The Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-DTJsa07kRg",
        "colab_type": "code",
        "outputId": "55237f2e-5592-4ab8-d91d-00c03542f658",
        "colab": {}
      },
      "source": [
        "# Not Working\n",
        "# define the model\n",
        "model = define_model(vocab_size, max_length)\n",
        "# define checkpoint callback\n",
        "filepath = folderName+'/'+'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "# fit model\n",
        "model.fit([X1train, X2train], ytrain, epochs=10, verbose=2, callbacks=[checkpoint], validation_data=([X1test, X2test], ytest))\n",
        "print('Model Fitting Done Successfully')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 34)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_1 (InputLayer)            (None, 4096)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 34, 256)      1940224     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 4096)         0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 34, 256)      0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 256)          1048832     dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 256)          525312      dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 256)          0           dense_1[0][0]                    \n",
            "                                                                 lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 7579)         1947803     dense_2[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 5,527,963\n",
            "Trainable params: 5,527,963\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Train on 306404 samples, validate on 50903 samples\n",
            "Epoch 1/20\n",
            " - 668s - loss: 4.5363 - val_loss: 4.0905\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 4.09045, saving model to /storage/Flickr_Data/model-ep001-loss4.536-val_loss4.090.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBN1XJyAVHZz",
        "colab_type": "code",
        "outputId": "8cab16d1-dc52-4112-d1cb-5b9997808aa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        }
      },
      "source": [
        "# load training dataset (6K)\n",
        "filename = folderName+'/Flickr_TextData/Flickr_8k.trainImages.txt'\n",
        "train = load_set(filename)\n",
        "print('Dataset: %d' % len(train))\n",
        "# descriptions\n",
        "train_descriptions = load_clean_descriptions(folderName+'/descriptions.txt', train)\n",
        "print('Descriptions: train=%d' % len(train_descriptions))\n",
        "# photo features\n",
        "train_features = load_photo_features(folderName+'/features.pkl', train)\n",
        "print('Photos: train=%d' % len(train_features))\n",
        "# prepare tokenizer\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "# determine the maximum sequence length\n",
        "max_length = maximum_length(train_descriptions)\n",
        "print('Description Length: %d' % max_length)\n",
        " \n",
        "# define the model\n",
        "model = define_model(vocab_size, max_length)\n",
        "# train the model, run epochs manually and save after each epoch\n",
        "epochs = 10\n",
        "steps = len(train_descriptions)\n",
        "for i in range(epochs):\n",
        "\t# create the data generator\n",
        "\tgenerator = data_generator(train_descriptions, train_features, tokenizer, max_length, vocab_size)\n",
        "\t# fit for one epoch\n",
        "\tmodel.fit_generator(generator, epochs=i, steps_per_epoch=steps, verbose=1)\n",
        "\t# save model\n",
        "  #filepath = folderName+'/'+'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
        "\tmodel.save(folderName+'/'+'model_' + str(i) + '.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 6000\n",
            "Descriptions: train=6000\n",
            "Photos: train=6000\n",
            "Vocabulary Size: 7579\n",
            "Description Length: 34\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            (None, 34)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            (None, 4096)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 34, 256)      1940224     input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 4096)         0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 34, 256)      0           embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 256)          1048832     dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   (None, 256)          525312      dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 256)          0           dense_4[0][0]                    \n",
            "                                                                 lstm_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 256)          65792       add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 7579)         1947803     dense_5[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 5,527,963\n",
            "Trainable params: 5,527,963\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 2377s 396ms/step - loss: 4.6603\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 2374s 396ms/step - loss: 3.9018\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 2356s 393ms/step - loss: 3.6479\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 2258s 376ms/step - loss: 3.5098\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 2248s 375ms/step - loss: 3.4147\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 2282s 380ms/step - loss: 3.3515\n",
            "Epoch 1/1\n",
            "4783/6000 [======================>.......] - ETA: 7:42 - loss: 3.3046"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DedpORz27kRl",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbewRg4s7kRm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21zDZBET7kRr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate a description for an image\n",
        "def generate_desc(model, tokenizer, photo, max_length):\n",
        "    # seed the generation process\n",
        "    in_text = 'startseq'\n",
        "    # iterate over the whole length of the sequence\n",
        "    for i in range(max_length):\n",
        "        # integer encode input sequence\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        # pad input\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        # predict next word\n",
        "        yhat = model.predict([photo,sequence], verbose=0)\n",
        "        # convert probability to integer\n",
        "        yhat = argmax(yhat)\n",
        "        # map integer to word\n",
        "        word = word_for_id(yhat, tokenizer)\n",
        "        # stop if we cannot map the word\n",
        "        if word is None:\n",
        "            break\n",
        "        # append as input for generating the next word\n",
        "        in_text += ' ' + word\n",
        "        # stop if we predict the end of the sequence\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    return in_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2S5RbYb7kRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
        "    actual, predicted = list(), list()\n",
        "    # step over the whole set\n",
        "    for key, desc_list in descriptions.items():\n",
        "        # generate description\n",
        "        yhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
        "        # store actual and predicted\n",
        "        references = [d.split() for d in desc_list]\n",
        "        actual.append(references)\n",
        "        predicted.append(yhat.split())\n",
        "    # calculate BLEU score\n",
        "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBJbMi8QUnpy",
        "colab_type": "code",
        "outputId": "0af489b6-b1da-428e-aa09-ce70ccef5510",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# prepare tokenizer on train set\n",
        " \n",
        "# load training dataset (6K)\n",
        "filename = folderName+'/Flickr_TextData/Flickr_8k.trainImages.txt'\n",
        "train = load_set(filename)\n",
        "print('Dataset: %d' % len(train))\n",
        "# descriptions\n",
        "train_descriptions = load_clean_descriptions(folderName+'/descriptions.txt', train)\n",
        "print('Descriptions: train=%d' % len(train_descriptions))\n",
        "# prepare tokenizer\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "# determine the maximum sequence length\n",
        "max_length = maximum_length(train_descriptions)\n",
        "print('Description Length: %d' % max_length)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 6000\n",
            "Descriptions: train=6000\n",
            "Vocabulary Size: 7579\n",
            "Description Length: 34\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3uwO0jCU0Cy",
        "colab_type": "code",
        "outputId": "9b559ec7-2315-4683-999b-3ef61f64c612",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# prepare test set\n",
        " \n",
        "# load test set\n",
        "filename = folderName+'/Flickr_TextData/Flickr_8k.testImages.txt'\n",
        "test = load_set(filename)\n",
        "print('Dataset: %d' % len(test))\n",
        "# descriptions\n",
        "test_descriptions = load_clean_descriptions(folderName+'/descriptions.txt', test)\n",
        "print('Descriptions: test=%d' % len(test_descriptions))\n",
        "# photo features\n",
        "test_features = load_photo_features(folderName+'/features.pkl', test)\n",
        "print('Photos: test=%d' % len(test_features))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 1000\n",
            "Descriptions: test=1000\n",
            "Photos: test=1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLbQChM87kRy",
        "colab_type": "code",
        "outputId": "3d0fd37a-8db3-41bf-c484-d208d5f838d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "# load the model\n",
        "#filename = folderName+'/'+'model-ep001-loss4.523-val_loss4.065.h5'\n",
        "filename = folderName+'/model_7.h5'\n",
        "model = load_model(filename)\n",
        "# evaluate model\n",
        "evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3239: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "BLEU-1: 0.527713\n",
            "BLEU-2: 0.277879\n",
            "BLEU-3: 0.186908\n",
            "BLEU-4: 0.085236\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC_3y_lLVBKY",
        "colab_type": "text"
      },
      "source": [
        "# Generate New Captions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irVleiMZU8gC",
        "colab_type": "code",
        "outputId": "f6f25b27-71eb-4560-871c-6c948b51e93e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# load training dataset (6K)\n",
        "filename = folderName+'/Flickr_TextData/Flickr_8k.trainImages.txt'\n",
        "train = load_set(filename)\n",
        "print('Dataset: %d' % len(train))\n",
        "# descriptions\n",
        "train_descriptions = load_clean_descriptions(folderName+'/descriptions.txt', train)\n",
        "print('Descriptions: train=%d' % len(train_descriptions))\n",
        "# prepare tokenizer\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "# save the tokenizer\n",
        "dump(tokenizer, open(folderName+'/tokenizer.pkl', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 6000\n",
            "Descriptions: train=6000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQ65ld_VWdTU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# extract features from each photo in the directory\n",
        "def extract_features(filename):\n",
        "\t# load the model\n",
        "\tmodel = VGG16()\n",
        "\t# re-structure the model\n",
        "\tmodel.layers.pop()\n",
        "\tmodel = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
        "\t# load the photo\n",
        "\timage = load_img(filename, target_size=(224, 224))\n",
        "\t# convert the image pixels to a numpy array\n",
        "\timage = img_to_array(image)\n",
        "\t# reshape data for the model\n",
        "\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "\t# prepare the image for the VGG model\n",
        "\timage = preprocess_input(image)\n",
        "\t# get features\n",
        "\tfeature = model.predict(image, verbose=0)\n",
        "\treturn feature"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuxy0B507kR1",
        "colab_type": "code",
        "outputId": "71ab0e78-47b1-4c8e-b6fd-52249a7569ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# load the tokenizer\n",
        "tokenizer = load(open(folderName+'/tokenizer.pkl', 'rb'))\n",
        "# pre-define the max sequence length (from training)\n",
        "max_length = 34\n",
        "# load the model\n",
        "filename = folderName+'/model_7.h5'\n",
        "model = load_model(filename)\n",
        "# load and prepare the photograph\n",
        "imageFileName = imageDir+'/10815824_2997e03d76.jpg'\n",
        "photo = extract_features(imageFileName)\n",
        "# generate description\n",
        "description = generate_desc(model, tokenizer, photo, max_length)\n",
        "print(description)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "startseq man in black shirt is walking on the street endseq\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_cICaRzF5O5",
        "colab_type": "text"
      },
      "source": [
        "#  Fit Dev Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63st6lv_VK_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split a dataset into train/test elements\n",
        "def train_test_split(dataset):\n",
        "\t# order keys so the split is consistent\n",
        "\tordered = sorted(dataset)\n",
        "\t# return split dataset as two new sets\n",
        "\treturn set(ordered[:100]), set(ordered[100:200])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTwyCINBF2uQ",
        "colab_type": "code",
        "outputId": "a91dd630-f695-4dba-e883-b1b00c8024cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# load dev set\n",
        "filename = folderName+'/Flickr_TextData/Flickr_8k.devImages.txt'\n",
        "dataset = load_set(filename)\n",
        "print('Dataset: %d' % len(dataset))\n",
        "# train-test split\n",
        "train, test = train_test_split(dataset)\n",
        "print('Train=%d, Test=%d' % (len(train), len(test)))\n",
        "# descriptions\n",
        "train_descriptions = load_clean_descriptions(folderName+'/descriptions.txt', train)\n",
        "test_descriptions = load_clean_descriptions(folderName+'/descriptions.txt', test)\n",
        "print('Descriptions: train=%d, test=%d' % (len(train_descriptions), len(test_descriptions)))\n",
        "# photo features\n",
        "train_features = load_photo_features(folderName+'/features.pkl', train)\n",
        "test_features = load_photo_features(folderName+'/features.pkl', test)\n",
        "print('Photos: train=%d, test=%d' % (len(train_features), len(test_features)))\n",
        "# prepare tokenizer\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "# determine the maximum sequence length\n",
        "max_length = maximum_length(train_descriptions)\n",
        "print('Description Length: %d' % max_length)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 1000\n",
            "Train=100, Test=100\n",
            "Descriptions: train=100, test=100\n",
            "Photos: train=100, test=100\n",
            "Vocabulary Size: 876\n",
            "Description Length: 31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qp_CY2RDbpF8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluate the skill of the model\n",
        "def evaluate_model_new(model, descriptions, photos, tokenizer, max_length):\n",
        "\tactual, predicted = list(), list()\n",
        "\t# step over the whole set\n",
        "\tfor key, desc in descriptions.items():\n",
        "\t\t# generate description\n",
        "\t\tyhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
        "\t\t# store actual and predicted\n",
        "\t\tactual.append([d.split() for d in desc])\n",
        "\t\tpredicted.append(yhat.split())\n",
        "\t# calculate BLEU score\n",
        "\tbleu = corpus_bleu(actual, predicted)\n",
        "\treturn bleu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXFFPEqXGxrn",
        "colab_type": "code",
        "outputId": "b612abce-4c4a-4497-80c1-40aefea55d8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from pandas import DataFrame\n",
        "# define experiment\n",
        "model_name = 'baseline1'\n",
        "verbose = 1\n",
        "n_epochs = 10\n",
        "n_photos_per_update = 2\n",
        "n_batches_per_epoch = int(len(train) / n_photos_per_update)\n",
        "n_repeats = 3\n",
        " \n",
        "# run experiment\n",
        "train_results, test_results = list(), list()\n",
        "for i in range(n_repeats):\n",
        "  # define the model\n",
        "  model = define_model(vocab_size, max_length)\n",
        "  # fit model\n",
        "  #data_generator(train_descriptions, train_features, tokenizer, max_length, n_photos_per_update\n",
        "  model.fit_generator(data_generator(train_descriptions, train_features, tokenizer, max_length, vocab_size), steps_per_epoch=n_batches_per_epoch, epochs=n_epochs, verbose=verbose)\n",
        "  # evaluate model on training data\n",
        "  train_score = evaluate_model_new(model, train_descriptions, train_features, tokenizer, max_length)\n",
        "  test_score = evaluate_model_new(model, test_descriptions, test_features, tokenizer, max_length)\n",
        "  print(\"train_score:\", train_score)\n",
        "  print(\"test_score:\", test_score)\n",
        "  # store\n",
        "  train_results.append(train_score)\n",
        "  test_results.append(test_score)\n",
        "  #print('>%d: train=%f test=%f' % ((i+1), train_score, test_score))\n",
        "# save results to file\n",
        "df = DataFrame()\n",
        "df['train'] = train_results\n",
        "df['test'] = test_results\n",
        "print(df.describe())\n",
        "df.to_csv(folderName+'/'+model_name+'.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_12\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_22 (InputLayer)           (None, 31)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_21 (InputLayer)           (None, 4096)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_10 (Embedding)        (None, 31, 256)      224256      input_22[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 4096)         0           input_21[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_20 (Dropout)            (None, 31, 256)      0           embedding_10[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_28 (Dense)                (None, 256)          1048832     dropout_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lstm_10 (LSTM)                  (None, 256)          525312      dropout_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 256)          0           dense_28[0][0]                   \n",
            "                                                                 lstm_10[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_29 (Dense)                (None, 256)          65792       add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_30 (Dense)                (None, 876)          225132      dense_29[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 2,089,324\n",
            "Trainable params: 2,089,324\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "50/50 [==============================] - 21s 419ms/step - loss: 5.9996\n",
            "Epoch 2/10\n",
            "50/50 [==============================] - 17s 331ms/step - loss: 5.7303\n",
            "Epoch 3/10\n",
            "50/50 [==============================] - 17s 340ms/step - loss: 5.2158\n",
            "Epoch 4/10\n",
            "50/50 [==============================] - 17s 334ms/step - loss: 5.2408\n",
            "Epoch 5/10\n",
            "50/50 [==============================] - 17s 346ms/step - loss: 4.9362\n",
            "Epoch 6/10\n",
            "50/50 [==============================] - 17s 336ms/step - loss: 4.8920\n",
            "Epoch 7/10\n",
            "50/50 [==============================] - 18s 351ms/step - loss: 4.6647\n",
            "Epoch 8/10\n",
            "50/50 [==============================] - 17s 332ms/step - loss: 4.7789\n",
            "Epoch 9/10\n",
            "50/50 [==============================] - 17s 345ms/step - loss: 4.2455\n",
            "Epoch 10/10\n",
            "50/50 [==============================] - 17s 335ms/step - loss: 4.2573\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train_score: 0.016021736418406415\n",
            "test_score: 0.07087703460592933\n",
            "Model: \"model_13\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_24 (InputLayer)           (None, 31)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_23 (InputLayer)           (None, 4096)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_11 (Embedding)        (None, 31, 256)      224256      input_24[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_21 (Dropout)            (None, 4096)         0           input_23[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_22 (Dropout)            (None, 31, 256)      0           embedding_11[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_31 (Dense)                (None, 256)          1048832     dropout_21[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lstm_11 (LSTM)                  (None, 256)          525312      dropout_22[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 256)          0           dense_31[0][0]                   \n",
            "                                                                 lstm_11[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_32 (Dense)                (None, 256)          65792       add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_33 (Dense)                (None, 876)          225132      dense_32[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 2,089,324\n",
            "Trainable params: 2,089,324\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "50/50 [==============================] - 22s 434ms/step - loss: 6.0382\n",
            "Epoch 2/10\n",
            "50/50 [==============================] - 17s 337ms/step - loss: 5.7323\n",
            "Epoch 3/10\n",
            "50/50 [==============================] - 17s 348ms/step - loss: 5.2509\n",
            "Epoch 4/10\n",
            "50/50 [==============================] - 17s 335ms/step - loss: 5.2622\n",
            "Epoch 5/10\n",
            "50/50 [==============================] - 18s 353ms/step - loss: 4.9148\n",
            "Epoch 6/10\n",
            "50/50 [==============================] - 17s 334ms/step - loss: 4.9551\n",
            "Epoch 7/10\n",
            "50/50 [==============================] - 17s 340ms/step - loss: 4.5745\n",
            "Epoch 8/10\n",
            "50/50 [==============================] - 16s 328ms/step - loss: 4.6215\n",
            "Epoch 9/10\n",
            "50/50 [==============================] - 17s 338ms/step - loss: 4.2859\n",
            "Epoch 10/10\n",
            "50/50 [==============================] - 16s 330ms/step - loss: 4.2607\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train_score: 0.07589270987846462\n",
            "test_score: 0.04812111609779902\n",
            "Model: \"model_14\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_26 (InputLayer)           (None, 31)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_25 (InputLayer)           (None, 4096)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_12 (Embedding)        (None, 31, 256)      224256      input_26[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_23 (Dropout)            (None, 4096)         0           input_25[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_24 (Dropout)            (None, 31, 256)      0           embedding_12[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_34 (Dense)                (None, 256)          1048832     dropout_23[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lstm_12 (LSTM)                  (None, 256)          525312      dropout_24[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 256)          0           dense_34[0][0]                   \n",
            "                                                                 lstm_12[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_35 (Dense)                (None, 256)          65792       add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_36 (Dense)                (None, 876)          225132      dense_35[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 2,089,324\n",
            "Trainable params: 2,089,324\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "50/50 [==============================] - 21s 428ms/step - loss: 6.0864\n",
            "Epoch 2/10\n",
            "50/50 [==============================] - 17s 334ms/step - loss: 5.7293\n",
            "Epoch 3/10\n",
            "50/50 [==============================] - 17s 339ms/step - loss: 5.2363\n",
            "Epoch 4/10\n",
            "50/50 [==============================] - 17s 337ms/step - loss: 5.2021\n",
            "Epoch 5/10\n",
            "50/50 [==============================] - 17s 337ms/step - loss: 4.9396\n",
            "Epoch 6/10\n",
            "50/50 [==============================] - 17s 331ms/step - loss: 4.9223\n",
            "Epoch 7/10\n",
            "50/50 [==============================] - 17s 343ms/step - loss: 4.6435\n",
            "Epoch 8/10\n",
            "50/50 [==============================] - 17s 332ms/step - loss: 4.5422\n",
            "Epoch 9/10\n",
            "50/50 [==============================] - 17s 344ms/step - loss: 4.2795\n",
            "Epoch 10/10\n",
            "50/50 [==============================] - 17s 331ms/step - loss: 4.1883\n",
            "train_score: 0.01995796688766336\n",
            "test_score: 0.08743590158413432\n",
            "          train      test\n",
            "count  3.000000  3.000000\n",
            "mean   0.037291  0.068811\n",
            "std    0.033488  0.019739\n",
            "min    0.016022  0.048121\n",
            "25%    0.017990  0.059499\n",
            "50%    0.019958  0.070877\n",
            "75%    0.047925  0.079156\n",
            "max    0.075893  0.087436\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-hwA1EuoWoc",
        "colab_type": "code",
        "outputId": "7bc7eb9e-0daa-49f5-95f7-8f44b8422eca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "from pandas import read_csv\n",
        "from matplotlib import pyplot\n",
        " \n",
        "# load all .csv results into a dataframe\n",
        "train, test = DataFrame(), DataFrame()\n",
        "#directory = 'results'\n",
        "directory = folderName\n",
        "for name in listdir(directory):\n",
        "\tif not name.endswith('csv'):\n",
        "\t\tcontinue\n",
        "\tfilename = directory + '/' + name\n",
        "\tdata = read_csv(filename, header=0)\n",
        "\texperiment = name.split('.')[0]\n",
        "\ttrain[experiment] = data['train']\n",
        "\ttest[experiment] = data['test']\n",
        " \n",
        "# plot results on train\n",
        "train.boxplot(vert=False)\n",
        "pyplot.show()\n",
        "# plot results on test\n",
        "test.boxplot(vert=False)\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAD8CAYAAABKKbKtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAD8CAYAAABKKbKtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}